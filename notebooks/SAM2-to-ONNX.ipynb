{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52dacc2d-6566-4c49-beaa-ea04407e4fa0",
   "metadata": {},
   "source": [
    "# Download official SAM2 weights and modeling code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86df18d1-89a4-4fba-bb50-a30ecd2d5656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-01-14 23:00:34--  https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_tiny.pt\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 18.154.161.79, 18.154.161.30, 18.154.161.85, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|18.154.161.79|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 155906050 (149M) [application/vnd.snesdev-page-table]\n",
      "Saving to: ‘sam2_hiera_tiny.pt’\n",
      "\n",
      "sam2_hiera_tiny.pt  100%[===================>] 148.68M  5.61MB/s    in 25s     \n",
      "\n",
      "2025-01-14 23:00:59 (5.97 MB/s) - ‘sam2_hiera_tiny.pt’ saved [155906050/155906050]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_tiny.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15aabdae",
   "metadata": {},
   "source": [
    "# Install dependency with uv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b442560-5250-4280-8213-6bb58544459a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uv 0.5.6 (b70c4f30e 2024-12-03)\n",
      "Using CPython \u001b[36m3.11.11\u001b[39m\n",
      "Creating virtual environment at: \u001b[36m.venv\u001b[39m\n",
      "Activate with: \u001b[32msource .venv/bin/activate\u001b[39m\n",
      "\u001b[2mResolved \u001b[1m81 packages\u001b[0m \u001b[2min 5ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m61 packages\u001b[0m \u001b[2min 453ms\u001b[0m\u001b[0m                              \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mantlr4-python3-runtime\u001b[0m\u001b[2m==4.9.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mappnope\u001b[0m\u001b[2m==0.1.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1masttokens\u001b[0m\u001b[2m==3.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcoloredlogs\u001b[0m\u001b[2m==15.0.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcomm\u001b[0m\u001b[2m==0.2.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdebugpy\u001b[0m\u001b[2m==1.8.11\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdecorator\u001b[0m\u001b[2m==5.1.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mexecuting\u001b[0m\u001b[2m==2.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfilelock\u001b[0m\u001b[2m==3.16.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mflatbuffers\u001b[0m\u001b[2m==24.12.23\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2024.12.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhumanfriendly\u001b[0m\u001b[2m==10.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhydra-core\u001b[0m\u001b[2m==1.3.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1miopath\u001b[0m\u001b[2m==0.1.10\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mipykernel\u001b[0m\u001b[2m==6.29.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mipython\u001b[0m\u001b[2m==8.31.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjedi\u001b[0m\u001b[2m==0.19.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjinja2\u001b[0m\u001b[2m==3.1.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjupyter-client\u001b[0m\u001b[2m==8.6.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjupyter-core\u001b[0m\u001b[2m==5.7.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmarkdown-it-py\u001b[0m\u001b[2m==3.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmarkupsafe\u001b[0m\u001b[2m==3.0.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmatplotlib-inline\u001b[0m\u001b[2m==0.1.7\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmdurl\u001b[0m\u001b[2m==0.1.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mml-dtypes\u001b[0m\u001b[2m==0.5.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmpmath\u001b[0m\u001b[2m==1.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnest-asyncio\u001b[0m\u001b[2m==1.6.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnetworkx\u001b[0m\u001b[2m==3.4.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.2.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1momegaconf\u001b[0m\u001b[2m==2.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1monnx\u001b[0m\u001b[2m==1.17.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1monnxruntime\u001b[0m\u001b[2m==1.20.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1monnxscript\u001b[0m\u001b[2m==0.1.0.dev20250114\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1monnxsim\u001b[0m\u001b[2m==0.4.36\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpackaging\u001b[0m\u001b[2m==24.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mparso\u001b[0m\u001b[2m==0.8.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpexpect\u001b[0m\u001b[2m==4.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpillow\u001b[0m\u001b[2m==11.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mplatformdirs\u001b[0m\u001b[2m==4.3.6\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mportalocker\u001b[0m\u001b[2m==3.1.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mprompt-toolkit\u001b[0m\u001b[2m==3.0.48\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mprotobuf\u001b[0m\u001b[2m==5.29.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpsutil\u001b[0m\u001b[2m==6.1.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mptyprocess\u001b[0m\u001b[2m==0.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpure-eval\u001b[0m\u001b[2m==0.2.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpygments\u001b[0m\u001b[2m==2.19.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpython-dateutil\u001b[0m\u001b[2m==2.9.0.post0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyyaml\u001b[0m\u001b[2m==6.0.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyzmq\u001b[0m\u001b[2m==26.2.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrich\u001b[0m\u001b[2m==13.9.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msam-2\u001b[0m\u001b[2m==1.0 (from git+https://github.com/facebookresearch/segment-anything-2.git@2b90b9f5ceec907a1c18123530e92e794ad901a4)\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msix\u001b[0m\u001b[2m==1.17.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mstack-data\u001b[0m\u001b[2m==0.6.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msympy\u001b[0m\u001b[2m==1.13.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.5.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.20.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtornado\u001b[0m\u001b[2m==6.4.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtqdm\u001b[0m\u001b[2m==4.67.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtraitlets\u001b[0m\u001b[2m==5.14.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyping-extensions\u001b[0m\u001b[2m==4.12.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwcwidth\u001b[0m\u001b[2m==0.2.13\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv --version\n",
    "!uv venv\n",
    "!uv sync"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28bce33-5c85-4d0a-b5e6-86cce2024593",
   "metadata": {},
   "source": [
    "# Split model into Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4171d572-07c7-498f-9b62-d5ae3de6a4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple, Any\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.init import trunc_normal_\n",
    "\n",
    "from sam2.modeling.sam2_base import SAM2Base\n",
    "\n",
    "class SAM2ImageEncoder(nn.Module):\n",
    "    def __init__(self, sam_model: SAM2Base) -> None:\n",
    "        super().__init__()\n",
    "        self.model = sam_model\n",
    "        self.image_encoder = sam_model.image_encoder\n",
    "        self.no_mem_embed = sam_model.no_mem_embed\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple[Any, Any, Any]:\n",
    "        backbone_out = self.image_encoder(x)\n",
    "        backbone_out[\"backbone_fpn\"][0] = self.model.sam_mask_decoder.conv_s0(\n",
    "            backbone_out[\"backbone_fpn\"][0]\n",
    "        )\n",
    "        backbone_out[\"backbone_fpn\"][1] = self.model.sam_mask_decoder.conv_s1(\n",
    "            backbone_out[\"backbone_fpn\"][1]\n",
    "        )\n",
    "\n",
    "        feature_maps = backbone_out[\"backbone_fpn\"][-self.model.num_feature_levels:]\n",
    "        vision_pos_embeds = backbone_out[\"vision_pos_enc\"][-self.model.num_feature_levels:]\n",
    "\n",
    "        feat_sizes = [(x.shape[-2], x.shape[-1]) for x in vision_pos_embeds]\n",
    "\n",
    "        # flatten NxCxHxW to HWxNxC\n",
    "        vision_feats = [x.flatten(2).permute(2, 0, 1) for x in feature_maps]\n",
    "        vision_pos_embeds = [x.flatten(2).permute(2, 0, 1) for x in vision_pos_embeds]\n",
    "\n",
    "        vision_feats[-1] = vision_feats[-1] + self.no_mem_embed\n",
    "\n",
    "        feats = [feat.permute(1, 2, 0).reshape(1, -1, *feat_size)\n",
    "                 for feat, feat_size in zip(vision_feats[::-1], feat_sizes[::-1])][::-1]\n",
    "\n",
    "        return feats[0], feats[1], feats[2]\n",
    "\n",
    "\n",
    "class SAM2ImageDecoder(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            sam_model: SAM2Base,\n",
    "            multimask_output: bool\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.mask_decoder = sam_model.sam_mask_decoder\n",
    "        self.prompt_encoder = sam_model.sam_prompt_encoder\n",
    "        self.model = sam_model\n",
    "        self.multimask_output = multimask_output\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(\n",
    "            self,\n",
    "            image_embed: torch.Tensor,\n",
    "            high_res_feats_0: torch.Tensor,\n",
    "            high_res_feats_1: torch.Tensor,\n",
    "            point_coords: torch.Tensor,\n",
    "            point_labels: torch.Tensor,\n",
    "            mask_input: torch.Tensor,\n",
    "            has_mask_input: torch.Tensor,\n",
    "            img_size: torch.Tensor\n",
    "    ):\n",
    "        sparse_embedding = self._embed_points(point_coords, point_labels)\n",
    "        self.sparse_embedding = sparse_embedding\n",
    "        dense_embedding = self._embed_masks(mask_input, has_mask_input)\n",
    "\n",
    "        high_res_feats = [high_res_feats_0, high_res_feats_1]\n",
    "        image_embed = image_embed\n",
    "\n",
    "        masks, iou_predictions, _, _ = self.mask_decoder.predict_masks(\n",
    "            image_embeddings=image_embed,\n",
    "            image_pe=self.prompt_encoder.get_dense_pe(),\n",
    "            sparse_prompt_embeddings=sparse_embedding,\n",
    "            dense_prompt_embeddings=dense_embedding,\n",
    "            repeat_image=False,\n",
    "            high_res_features=high_res_feats,\n",
    "        )\n",
    "\n",
    "        if self.multimask_output:\n",
    "            masks = masks[:, 1:, :, :]\n",
    "            iou_predictions = iou_predictions[:, 1:]\n",
    "        else:\n",
    "            masks, iou_predictions = self.mask_decoder._dynamic_multimask_via_stability(masks, iou_predictions)\n",
    "\n",
    "        masks = torch.clamp(masks, -32.0, 32.0)\n",
    "        print(masks.shape, iou_predictions.shape)\n",
    "\n",
    "        masks = F.interpolate(masks, (img_size[0], img_size[1]), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "        return masks, iou_predictions\n",
    "\n",
    "    def _embed_points(self, point_coords: torch.Tensor, point_labels: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        point_coords = point_coords + 0.5\n",
    "\n",
    "        padding_point = torch.zeros((point_coords.shape[0], 1, 2), device=point_coords.device)\n",
    "        padding_label = -torch.ones((point_labels.shape[0], 1), device=point_labels.device)\n",
    "        point_coords = torch.cat([point_coords, padding_point], dim=1)\n",
    "        point_labels = torch.cat([point_labels, padding_label], dim=1)\n",
    "\n",
    "        point_coords[:, :, 0] = point_coords[:, :, 0] / self.model.image_size\n",
    "        point_coords[:, :, 1] = point_coords[:, :, 1] / self.model.image_size\n",
    "\n",
    "        point_embedding = self.prompt_encoder.pe_layer._pe_encoding(point_coords)\n",
    "        point_labels = point_labels.unsqueeze(-1).expand_as(point_embedding)\n",
    "\n",
    "        point_embedding = point_embedding * (point_labels != -1)\n",
    "        point_embedding = point_embedding + self.prompt_encoder.not_a_point_embed.weight * (\n",
    "                point_labels == -1\n",
    "        )\n",
    "\n",
    "        for i in range(self.prompt_encoder.num_point_embeddings):\n",
    "            point_embedding = point_embedding + self.prompt_encoder.point_embeddings[i].weight * (point_labels == i)\n",
    "\n",
    "        return point_embedding\n",
    "\n",
    "    def _embed_masks(self, input_mask: torch.Tensor, has_mask_input: torch.Tensor) -> torch.Tensor:\n",
    "        mask_embedding = has_mask_input * self.prompt_encoder.mask_downscaling(input_mask)\n",
    "        mask_embedding = mask_embedding + (\n",
    "                1 - has_mask_input\n",
    "        ) * self.prompt_encoder.no_mask_embed.weight.reshape(1, -1, 1, 1)\n",
    "        return mask_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b4d668-cc9d-4bea-8134-665c14210c8a",
   "metadata": {},
   "source": [
    "# Convert model to .onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf618cb-3e3e-4f96-a1a4-7e4e7d32d36d",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "828045eb-5a3d-40f0-a615-dfc30e95595f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 256, 256])\n",
      "torch.Size([1, 64, 128, 128])\n",
      "torch.Size([1, 256, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giuseppeambrosio/Desktop/projects/vite-sam/notebooks/.venv/lib/python3.11/site-packages/sam2/modeling/backbones/utils.py:30: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if pad_h > 0 or pad_w > 0:\n",
      "/Users/giuseppeambrosio/Desktop/projects/vite-sam/notebooks/.venv/lib/python3.11/site-packages/sam2/modeling/backbones/utils.py:58: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if Hp > H or Wp > W:\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sam2.build_sam import build_sam2\n",
    "\n",
    "model_type = \"sam2_hiera_tiny\"\n",
    "model_cfg = \"sam2_hiera_t.yaml\"\n",
    "input_size = 1024 \n",
    "multimask_output = True\n",
    "\n",
    "sam2_checkpoint = f\"./{model_type}.pt\"\n",
    "sam2_model = build_sam2(model_cfg, sam2_checkpoint, device=\"cpu\")\n",
    "\n",
    "img=torch.randn(1, 3, input_size, input_size).cpu()\n",
    "\n",
    "sam2_encoder = SAM2ImageEncoder(sam2_model).cpu()\n",
    "high_res_feats_0, high_res_feats_1, image_embed = sam2_encoder(img)\n",
    "print(high_res_feats_0.shape)\n",
    "print(high_res_feats_1.shape)\n",
    "print(image_embed.shape)\n",
    "\n",
    "torch.onnx.export(sam2_encoder,\n",
    "      img,\n",
    "      f\"{model_type}_encoder.onnx\",\n",
    "      export_params=True,\n",
    "      opset_version=17,\n",
    "      do_constant_folding=True,\n",
    "      input_names = ['image'],\n",
    "      output_names = ['high_res_feats_0', 'high_res_feats_1', 'image_embed'],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26602e82-4e39-4a87-b8dd-d0836b640a86",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1be325f9-ba22-44a4-bb5f-acfe49101392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 (64, 64) [256, 256]\n",
      "torch.Size([1, 3, 256, 256]) torch.Size([1, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giuseppeambrosio/Desktop/projects/vite-sam/notebooks/.venv/lib/python3.11/site-packages/sam2/modeling/sam/mask_decoder.py:203: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert image_embeddings.shape[0] == tokens.shape[0]\n",
      "/Users/giuseppeambrosio/Desktop/projects/vite-sam/notebooks/.venv/lib/python3.11/site-packages/sam2/modeling/sam/mask_decoder.py:207: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  image_pe.size(0) == 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256]) torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "sam2_decoder = SAM2ImageDecoder(sam2_model, multimask_output=multimask_output).cpu()\n",
    "\n",
    "embed_dim = sam2_model.sam_prompt_encoder.embed_dim\n",
    "embed_size = (sam2_model.image_size // sam2_model.backbone_stride, sam2_model.image_size // sam2_model.backbone_stride)\n",
    "mask_input_size = [4 * x for x in embed_size]\n",
    "print(embed_dim, embed_size, mask_input_size)\n",
    "\n",
    "point_coords = torch.randint(low=0, high=input_size, size=(1, 5, 2), dtype=torch.float)\n",
    "point_labels = torch.randint(low=0, high=1, size=(1, 5), dtype=torch.float)\n",
    "mask_input = torch.randn(1, 1, *mask_input_size, dtype=torch.float)\n",
    "has_mask_input = torch.tensor([1], dtype=torch.float)\n",
    "orig_im_size = torch.tensor([input_size, input_size], dtype=torch.int32)\n",
    "\n",
    "masks, scores = sam2_decoder(image_embed, high_res_feats_0, high_res_feats_1, point_coords, point_labels, mask_input, has_mask_input, orig_im_size)\n",
    "\n",
    "torch.onnx.export(sam2_decoder,\n",
    "      (image_embed, high_res_feats_0, high_res_feats_1, point_coords, point_labels, mask_input, has_mask_input, orig_im_size),\n",
    "      f\"{model_type}_decoder.onnx\",\n",
    "      export_params=True,\n",
    "      opset_version=16,\n",
    "      do_constant_folding=True,\n",
    "      input_names = ['image_embed', 'high_res_feats_0', 'high_res_feats_1', 'point_coords', 'point_labels', 'mask_input', 'has_mask_input', 'orig_im_size'],\n",
    "      output_names = ['masks', 'iou_predictions'],\n",
    "      dynamic_axes = {\"point_coords\": {0: \"num_labels\", 1: \"num_points\"},\n",
    "                      \"point_labels\": {0: \"num_labels\", 1: \"num_points\"},\n",
    "                      \"mask_input\": {0: \"num_labels\"},\n",
    "                      \"has_mask_input\": {0: \"num_labels\"}\n",
    "      }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e41306-d443-4060-90ff-8beb39f37ba4",
   "metadata": {},
   "source": [
    "# Test exported models with `onnxruntime`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99a43dc-1077-46fc-8d64-734f88ffe366",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88e93dad-28e0-4caf-91a6-0b114791307f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "onnx_model = onnx.load(\"sam2_hiera_tiny_encoder.onnx\")\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7280b92-6010-48fd-9ba6-0aad26cba989",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-01-14 23:24:42.614163 [W:onnxruntime:, graph.cc:109 MergeShapeInfo] Error merging shape info for output. '/image_encoder/trunk/Concat_3_output_0' source:{4} target:{5}. Falling back to lenient merge.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "ort_sess = ort.InferenceSession('sam2_hiera_tiny_encoder.onnx', {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "451abb08-fb64-4a8c-bc05-cf6a51746a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "high_res_feats_0\n",
      "(1, 32, 256, 256)\n",
      "high_res_feats_1\n",
      "(1, 64, 128, 128)\n",
      "image_embed\n",
      "(1, 256, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "img = torch.randn(1, 3, input_size, input_size).cpu()\n",
    "\n",
    "outputs = ort_sess.run(None, {\n",
    "    'image': img.numpy(),\n",
    "})\n",
    "\n",
    "for i in range(len(outputs)):\n",
    "    print (ort_sess.get_outputs()[i].name)\n",
    "    print (outputs[i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d213286a-9633-4c8c-9743-5bab76ba386d",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5237ada8-9e49-49a3-9673-14eb16bf41bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = onnx.load(\"sam2_hiera_tiny_decoder.onnx\")\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecf5fefd-1bb5-4629-b212-4f96863860f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masks\n",
      "(1, 3, 1024, 1024)\n",
      "iou_predictions\n",
      "(1, 3)\n"
     ]
    }
   ],
   "source": [
    "ort_sess = ort.InferenceSession('sam2_hiera_tiny_decoder.onnx', {})\n",
    "\n",
    "outputs = ort_sess.run(None, {\n",
    "    'image_embed': image_embed.detach().numpy(), \n",
    "    'high_res_feats_0': high_res_feats_0.detach().numpy(), \n",
    "    'high_res_feats_1': high_res_feats_1.detach().numpy(), \n",
    "    'point_coords': point_coords.numpy(), \n",
    "    'point_labels': point_labels.numpy(), \n",
    "    'mask_input': mask_input.numpy(), \n",
    "    'has_mask_input': has_mask_input.numpy(), \n",
    "    'orig_im_size': orig_im_size.numpy()\n",
    "})\n",
    "\n",
    "for i in range(len(outputs)):\n",
    "    print (ort_sess.get_outputs()[i].name)\n",
    "    print (outputs[i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7929ff-28c0-43cb-bdec-1f614b7f85dd",
   "metadata": {},
   "source": [
    "# Convert Encoder model .onnx to .ort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7268cbb-cef3-4ad2-bb5c-0f5b1d004847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting models with optimization style 'Fixed' and level 'all'\n",
      "Converting optimized ONNX model /Users/giuseppeambrosio/Desktop/projects/vite-sam/notebooks/sam2_hiera_tiny_encoder.onnx to ORT format model /Users/giuseppeambrosio/Desktop/projects/vite-sam/notebooks/sam2_hiera_tiny_encoder.ort\n",
      "\u001b[0;93m2025-01-14 23:25:25.476227 [W:onnxruntime:, graph.cc:109 MergeShapeInfo] Error merging shape info for output. '/image_encoder/trunk/Concat_3_output_0' source:{4} target:{5}. Falling back to lenient merge.\u001b[m\n",
      "Converted 1/1 models successfully.\n",
      "Generating config file from ORT format models with optimization style 'Fixed' and level 'all'\n",
      "2025-01-14 23:25:26,213 ort_format_model.utils [INFO] - Created config in /Users/giuseppeambrosio/Desktop/projects/vite-sam/notebooks/sam2_hiera_tiny_encoder.required_operators.config\n",
      "Converting models with optimization style 'Runtime' and level 'all'\n",
      "Converting optimized ONNX model /Users/giuseppeambrosio/Desktop/projects/vite-sam/notebooks/sam2_hiera_tiny_encoder.onnx to ORT format model /Users/giuseppeambrosio/Desktop/projects/vite-sam/notebooks/sam2_hiera_tiny_encoder.with_runtime_opt.ort\n",
      "\u001b[0;93m2025-01-14 23:25:26.493528 [W:onnxruntime:, graph.cc:109 MergeShapeInfo] Error merging shape info for output. '/image_encoder/trunk/Concat_3_output_0' source:{4} target:{5}. Falling back to lenient merge.\u001b[m\n",
      "Converted 1/1 models successfully.\n",
      "Converting models again without runtime optimizations to generate a complete config file. These converted models are temporary and will be deleted.\n",
      "Converting optimized ONNX model /Users/giuseppeambrosio/Desktop/projects/vite-sam/notebooks/sam2_hiera_tiny_encoder.onnx to ORT format model /Users/giuseppeambrosio/Desktop/projects/vite-sam/notebooks/tmpdf4bp46a.without_runtime_opt/sam2_hiera_tiny_encoder.ort\n",
      "\u001b[0;93m2025-01-14 23:25:27.382859 [W:onnxruntime:, graph.cc:109 MergeShapeInfo] Error merging shape info for output. '/image_encoder/trunk/Concat_3_output_0' source:{4} target:{5}. Falling back to lenient merge.\u001b[m\n",
      "Converted 1/1 models successfully.\n",
      "Generating config file from ORT format models with optimization style 'Runtime' and level 'all'\n",
      "2025-01-14 23:25:28,061 ort_format_model.utils [INFO] - Created config in /Users/giuseppeambrosio/Desktop/projects/vite-sam/notebooks/sam2_hiera_tiny_encoder.required_operators.with_runtime_opt.config\n"
     ]
    }
   ],
   "source": [
    "!python3 -m onnxruntime.tools.convert_onnx_models_to_ort sam2_hiera_tiny_encoder.onnx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
